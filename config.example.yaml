# Data Storage
data:
  base_directory: "data"  # Directory where uploaded files will be stored
  supported_extensions:
    - ".pdf"
    - ".docx"
    - ".txt"
    - ".html"
    - ".pptx"
    - ".xlsx"

# Server Configuration
server:
  host: "0.0.0.0"  # Host to bind the server to
  port: 8000       # Port to run the server on
  log_level: "info"  # Logging level (debug, info, warning, error, critical)

# Authentication Configuration
auth:
  secret_key: "your-secret-key-here"  # Change this in production!
  algorithm: "HS256"
  access_token_expire_minutes: 30

# Models Configuration
models:
  llm:
    repo_id: "TheBloke/Llama-2-7B-Chat-GGUF"  # HuggingFace repository ID
    filename: "llama-2-7b-chat.Q4_K_M.gguf"   # Model filename
    temperature: 0.7        # Controls randomness in generation (0.0 to 1.0)
    max_tokens: 2000       # Maximum tokens in response
    context_window: 4096   # Context window size
    top_p: 0.95           # Nucleus sampling parameter
    n_batch: 512          # Batch size for processing
  embeddings:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"  # Embedding model
    model_kwargs:
      device: "cpu"  # Device to run on (cpu, cuda, mps)

# Vector Store Configuration
vector_store:
  chunk_size: 1000     # Size of text chunks for processing
  chunk_overlap: 200   # Overlap between chunks
  retrieval_k: 4       # Number of chunks to retrieve for each query

# RAG Configuration
rag:
  # Keep {context} and {question} placeholders as is - they will be replaced with actual values at runtime
  qa_template: | 
    Use the following pieces of context to answer the question at the end.
    If you don't know the answer based on the context, just say "I don't know" - do not try to make up an answer.
    If you do know the answer, provide it clearly and concisely, incorporating relevant information from all provided sources.
    Make sure to use all relevant information from the sources to give a complete answer.

    Context:
    {context}

    Question: {question}

    Answer: Let me help you with that. 